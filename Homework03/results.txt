1. model architecture -> searching for a comparable simple model

[256,256,],10
Training loss:  0.02440399
Training accuracy:  0.9919333333333333
Test loss:  0.07440772
Test accuarcy:  0.9803314696485623

[128,],10
Training loss:  0.07765455
Training accuracy:  0.9753
Test loss:  0.09981109
Test accuarcy:  0.970547124600639

[64,],10
Training loss:  0.13534482
Training accuracy:  0.9595166666666667
Test loss:  0.13793625
Test accuarcy:  0.9590654952076677

[128,16],10
Training loss:  0.059718695
Training accuracy:  0.9809166666666667
Test loss:  0.107176684
Test accuarcy:  0.9698482428115016

[64,64],10
Training loss:  0.08431962
Training accuracy:  0.9731166666666666
Test loss:  0.11820367
Test accuarcy:  0.9679512779552716

[16,128]
Training loss:  0.23447359
Training accuracy:  0.9291333333333334
Test loss:  0.22784665
Test accuarcy:  0.9324081469648562

[256,128]
Training loss:  0.03555931
Training accuracy:  0.9883
Test loss:  0.0774295
Test accuarcy:  0.9784345047923323

-> [128,] seems relatively comparable

2. Momentum
if momentum = 1 -> loss gets bigger
in between 0 and 1 worse than 0

3. learning rate
Most of the training is done in the first epoch, the rest is just finetuning
If we make the learning rate smaller, it takes a little longer until it converges

4. batch size
decreasing batch size: increases the time needed and decreased performance (tried 16)
Area between 32 and 64 seems optimal
increasing further: decreases the time needed even more but also slowly decreasing (tried 128,256)
