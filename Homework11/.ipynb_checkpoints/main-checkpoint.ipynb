{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1572ee-8302-46f3-9d80-a1f22a3286a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:55:14.411737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-15 14:55:14.540088: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-15 14:55:14.540107: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-15 14:55:15.104139: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 14:55:15.104234: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-15 14:55:15.104241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-15 14:55:15.877602: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-15 14:55:15.877645: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-15 14:55:15.877663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-0B8HQDM): /proc/driver/nvidia/version does not exist\n",
      "2023-02-15 14:55:15.877883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/prepared_HP1.txt\n",
      "  input_format: \n",
      "  model_prefix: tokenizer_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: data/prepared_HP1.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 7640 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=419971\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=30\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 7640 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 15386 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 7640\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 8238\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 8238 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5759 obj=9.37326 num_tokens=14588 num_tokens/piece=2.53308\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4804 obj=7.60729 num_tokens=14641 num_tokens/piece=3.04767\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3600 obj=7.62861 num_tokens=15967 num_tokens/piece=4.43528\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3599 obj=7.59088 num_tokens=15970 num_tokens/piece=4.43734\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2699 obj=7.83984 num_tokens=18273 num_tokens/piece=6.77029\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2699 obj=7.779 num_tokens=18275 num_tokens/piece=6.77103\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2200 obj=8.00447 num_tokens=20111 num_tokens/piece=9.14136\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2200 obj=7.9611 num_tokens=20113 num_tokens/piece=9.14227\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: tokenizer_model.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: tokenizer_model.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1593/1593 [05:09<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.6907975673675537\n",
      "accuracy: 0.3058576285839081\n",
      "Generated Text:  b'harry potter and!! said! she said! ron ron said i said and harry was said hermione. and and she she and she to was harry the hermione you'\n",
      "\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1593/1593 [05:12<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.023895025253296\n",
      "accuracy: 0.38567930459976196\n",
      "Generated Text:  b'harry potter and they were not! to bes still the still not be so still use so d so still said the. that as de the one harry as in'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import data_processing as dp\n",
    "import tensorflow as tf\n",
    "from model import MyModel\n",
    "from training_loop import training_loop\n",
    "\n",
    "# variables from prepare, do not change, or otherwise prepare newly\n",
    "original_file_path = \"data/HP1.txt\"\n",
    "prepared_file_path = \"data/prepared_HP1.txt\"\n",
    "model_prefix = 'tokenizer_model'\n",
    "# Define where to save the log and model\n",
    "config_name = \"run2_HP_100epochs\"\n",
    "train_summary_writer = tf.summary.create_file_writer(f\"logs/{config_name}/train\")\n",
    "VOCABULARY_SIZE = 2000 # 2000 - 7000\n",
    "WINDOW_SIZE = 32 # 32 - 256\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 100 # 64 - 256\n",
    "NUM_HEADS = 2 # 2-4\n",
    "FIRST_UNIT = 64 # 32-256\n",
    "\n",
    "starting_prompt = \"harry potter and \"\n",
    "EPOCHS_start = 0 # only needed if you want to continue training\n",
    "EPOCHS_end = 100 # 100 - 600\n",
    "TEST_OUTPUT_LENGTH = 30\n",
    "TOP_K = 20\n",
    "\n",
    "# important: tokenizer, optimizer, loss_function, VOCABULARY_SIZE, WINDOW_SIZE, EMBEDDING_DIM, NUM_HEADS, FIRST_UNIT\n",
    "# create file of model, add all the important data to it\n",
    "with open(f\"text/{config_name}.txt\", \"a\") as f:\n",
    "    f.write(f\"CONFIG: {config_name}\\nEPOCHS: {EPOCHS_start} - {EPOCHS_end}\\ndata: {prepared_file_path}\\n\\nvocabulary size: {VOCABULARY_SIZE}\\nwindow size: {WINDOW_SIZE}\\nembedding dim: {EMBEDDING_DIM}\\nnum heads: {NUM_HEADS}\\nfirst unit: {FIRST_UNIT}\\n\\n\")\n",
    "\n",
    "# variables for the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "# Data creation\n",
    "#**************\n",
    "\n",
    "# prepare if you want to create a new tokenizer and a new prepared data file\n",
    "data, prepared_data, tokenizer = dp.prepare_everything(original_file_path,prepared_file_path,model_prefix,VOCABULARY_SIZE)\n",
    "\n",
    "# if you only want to create a new tokenizer first, use loading afterwards\n",
    "# dp.train_tokenizer(prepared_file_path,VOCABULARY_SIZE,model_prefix)\n",
    "\n",
    "# load everything if already prepared\n",
    "#data, prepared_data, tokenizer = dp.load_everything(original_file_path,prepared_file_path,model_prefix)\n",
    "\n",
    "dataset = dp.create_dataset(prepared_data,tokenizer,WINDOW_SIZE, BATCH_SIZE)\n",
    "\n",
    "# Model and training\n",
    "#*******************\n",
    "\n",
    "model = MyModel(tokenizer, optimizer, loss_function, VOCABULARY_SIZE, WINDOW_SIZE, EMBEDDING_DIM, NUM_HEADS, FIRST_UNIT)\n",
    "#model.generate_text(\"hello dear,\", 1, 2)\n",
    "#model.load_weights(f'model/{config_name}')\n",
    "\n",
    "training_loop(model,dataset,EPOCHS_start, EPOCHS_end,starting_prompt, TEST_OUTPUT_LENGTH, TOP_K, train_summary_writer,config_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87bde8a-7bbb-413f-9e49-6cc86a1eb6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f49a5efc556cbed2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f49a5efc556cbed2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=\"logs/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
